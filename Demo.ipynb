{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tB-yLSEHN0OP"
      },
      "source": [
        "# üéÆ Pr√°tica de Aprendizado por Refor√ßo\n",
        "‚†Ä\n",
        "\n",
        "O objetivo deste notebook √© fazer uma breve demonstra√ß√£o da √°rea de Aprendizado por Refor√ßo utilizando um dos maiores cl√°ssicos da hist√≥ria dos video-games: ***Flappy-bird***.\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img align=\"center\"\n",
        "       src=\"https://github.com/Talendar/flappy-bird-gym/blob/main/imgs/yellow_bird_playing.gif?raw=true\"\n",
        "       width=\"200\"/>\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;\n",
        "  <img align=\"center\"\n",
        "       src=\"https://github.com/Talendar/flappy-bird-gym/blob/main/imgs/red_bird_start_screen.gif?raw=true\"\n",
        "       width=\"200\"/>\n",
        "  &nbsp;&nbsp;&nbsp;&nbsp;\n",
        "  <img align=\"center\"\n",
        "       src=\"https://github.com/Talendar/flappy-bird-gym/blob/main/imgs/blue_bird_playing.gif?raw=true\"\n",
        "       width=\"200\"/>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0j1FYo-N0OS"
      },
      "source": [
        "## üíª Programando..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C6sJ_iWN0OS"
      },
      "source": [
        "### Importando o Gymnasium\n",
        "\n",
        "O **[Gymnasium](https://gymnasium.farama.org/index.html)** √© uma biblioteca desenvolvida a partir de uma biblioteca semelhante desenvolvida pela OpenAI que cont√©m v√°rias implementa√ß√µes prontas de ambientes de Aprendizagem por Refor√ßo. Ela √© muito utilizada quando se quer testar um algoritmo de agente sem ter o trabalho de programar seu pr√≥prio ambiente.\n",
        "\n",
        "<img src=\"https://user-images.githubusercontent.com/10624937/42135602-b0335606-7d12-11e8-8689-dd1cf9fa11a9.gif\" alt=\"Exemplos de Ambientes do Gym\" class=\"inline\"/>\n",
        "<figcaption>Exemplo de Ambientes do Gymnasium</figcaption>\n",
        "<br>\n",
        "\n",
        "Para ter acesso a esses ambientes, basta importar o Gymnasium da seguinte forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdoCqR8RuWyc"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B6-cy8TmdtD"
      },
      "source": [
        "### O que √© um Ambiente?\n",
        "\n",
        "Um **Ambiente** de Aprendizagem por Refor√ßo √© um espa√ßo que representa o nosso problema, √© o objeto com o qual o nosso agente deve interagir para cumprir sua fun√ß√£o. Isso significa que o agente toma **a√ß√µes** nesse ambiente, e recebe **recompensas** dele com base na qualidade de sua tomada de decis√µes.\n",
        "\n",
        "Todos os ambientes s√£o dotados de um **espa√ßo de observa√ß√µes**, que √© a forma pela qual o agente recebe informa√ß√µes e deve se basear para a tomada de decis√µes, e um **espa√ßo de a√ß√µes**, que especifica as a√ß√µes poss√≠veis do agente. No xadrez, por exemplo, o espa√ßo de observa√ß√µes seria o conjunto de todas as configura√ß√µes diferentes do tabuleiro, e o espa√ßo de a√ß√µes seria o conjunto de todos os movimentos permitidos.\n",
        "\n",
        "<img src=\"https://www.raspberrypi.org/wp-content/uploads/2016/08/giphy-1-1.gif\" alt=\"Uma A√ß√£o do Xadrez\" class=\"inline\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWZ5pGuCN0OT"
      },
      "source": [
        "### Criando um Ambiente\n",
        "\n",
        "Para utilizar um dos ambientes do Gymnasium, n√≥s usamos a fun√ß√£o ```gym.make()```, passando o nome do ambiente desejado como par√¢metro e guardando o valor retornado em uma vari√°vel que chamaramos de ```env```. A lista com todos os ambientes do gym pode ser encontrada [aqui](https://gymnasium.farama.org/index.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiCGLjdxN0OT"
      },
      "outputs": [],
      "source": [
        "import flappy_bird_gymnasium\n",
        "\n",
        "env = gym.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDvQodjA_av9"
      },
      "source": [
        "Nesse caso, n√≥s vamos utilizar o ambiente ```FlappyBird-v0```, um ambiente que reproduz o jogo _Flappy Bird_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyRBNN0BmdtE"
      },
      "source": [
        "#### Caracter√≠sticas do Flappy Bird\n",
        "\n",
        "Antes de treinar qualquer agente, primeiro √© preciso entender melhor quais as caracter√≠sticas do nosso ambiente.\n",
        "\n",
        "O **Espa√ßo de Observa√ß√£o** √© definido por v√°rias informa√ß√µes lidas por um sensor, como: \n",
        "\n",
        "- A posi√ß√£o horizontal do √∫ltimo cano\n",
        "- A posi√ß√£o vertical do √∫ltimo cano superior\n",
        "- A posi√ß√£o vertical do √∫ltimo cano inferior\n",
        "- A posi√ß√£o horizontal do pr√≥ximo cano\n",
        "- A posi√ß√£o vertical do pr√≥ximo cano superior\n",
        "- A posi√ß√£o vertical do pr√≥ximo cano inferior\n",
        "- A posi√ß√£o horizontal do pr√≥ximo pr√≥ximo cano\n",
        "- A posi√ß√£o vertical do pr√≥ximo pr√≥ximo cano superior\n",
        "- A posi√ß√£o vertical do pr√≥ximo pr√≥ximo cano inferior\n",
        "- A posi√ß√£o vertical do jogador\n",
        "- A velocidade vertical do jogador\n",
        "- A rota√ß√£o do jogador\n",
        "\n",
        "Dessa forma, a cada instante recebemos uma lista da observa√ß√£o com o seguinte formato:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZA_-E2UE1ei",
        "outputId": "c3d41be8-5ad7-4e24-9af5-4e52b9751e0c"
      },
      "outputs": [],
      "source": [
        "print(env.observation_space.sample())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbvkKonLN0OT"
      },
      "source": [
        "#### Caracter√≠sticas do Flappy Bird\n",
        "\n",
        "Antes de treinar qualquer agente, primeiro √© preciso entender melhor quais as caracter√≠sticas do espa√ßo de a√ß√£o do pr√≥pio agente.\n",
        "\n",
        "O **Espa√ßo de A√ß√£o** √© definido por 2 informa√ß√µes:\n",
        "\n",
        "| Estado    | Informa√ß√£o                            |\n",
        "| :-------- | :------------------------------------ |\n",
        "| 0         | N√£o faz nada |\n",
        "| 1         | Bate as asas |\n",
        "\n",
        "Dessa forma, a cada instante recebemos uma lista da observa√ß√£o com o seguinte formato:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKhpz1HCN0OU",
        "outputId": "08a28b45-9e62-4628-9bb9-f76aecc0a189",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(env.action_space.sample())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FfBtSqFN0OU"
      },
      "source": [
        "Por fim, cada vez que tomamos uma a√ß√£o, recebemos do ambiente uma **recompensa**, conforme a tabela abaixo:\n",
        "\n",
        "| Ocorr√™ncia                       | Recompensa|\n",
        "| :--------------------------------| ---------:|\n",
        "| Estar vivo                       | $+0.1$    |\n",
        "| Passar por um cano com sucesso   | $+1.0$    |\n",
        "| Morrer                           | $-1.0$    |\n",
        "| Tocar o topo da tela             | $-0.5$    |\n",
        "\n",
        "O objetivo do jogo √© ultrapassar o maior n√∫mero poss√≠vel de canos. Assim, o dever do agente (p√°ssaro) √© acumular o m√°ximo de pontos poss√≠veis em um determinado per√≠odo de tempo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfmUyb0zN0OU"
      },
      "source": [
        "### ‚úç Testando o c√≥digo\n",
        "\n",
        "Agora que voc√™ j√° entende como o jogo funciona, vamos tentar aplicar esse conhecimento rodando um epis√≥dio do jogo tomando a√ß√µes aleat√≥rias!\n",
        "\n",
        "OBS: Algumas fun√ß√µes √∫teis do Gymnasium\n",
        "\n",
        "| M√©todo                 | Funcionalidade                                          |\n",
        "| :--------------------- |:------------------------------------------------------- |\n",
        "| `reset()`              | Inicializa o ambiente e recebe a observa√ß√£o inicial     |\n",
        "| `step(acao)`           | Executa uma a√ß√£o e recebe a observa√ß√£o e a recompensa   |\n",
        "| `render()`             | Renderiza o ambiente                                    |\n",
        "| `close()`              | Fecha o ambiente                                        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9alCOilXN0OU"
      },
      "outputs": [],
      "source": [
        "# Criando o ambiente\n",
        "env = gym.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)\n",
        "\n",
        "# Resete o ambiente e receba o estado inicial\n",
        "estado, _ = env.reset()\n",
        "\n",
        "# Inicializando uma vari√°vel booleana para indicar que o treinamento ainda n√£o foi conclu√≠do\n",
        "fim = False\n",
        "\n",
        "# Loop de treino\n",
        "while not fim:\n",
        "    # Escolha uma acao aleatoria\n",
        "    acao = env.action_space.sample()\n",
        "\n",
        "    # Tome essa acao e receba as informacoes do estado seguinte\n",
        "    prox_estado, recompensa, fim, truncated, info = env.step(acao)\n",
        "\n",
        "# Fechando o ambiente\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overview\n",
        "\n",
        "Para rodar uma partida, ou epis√≥dio de treinamento, s√£o necess√°rias algumas etapas:\n",
        "\n",
        "1. Iniciar um novo epis√≥dio chamando a fun√ß√£o ```reset()```\n",
        "2. Discretizar o estado\n",
        "3. Escolher uma a√ß√£o\n",
        "\n",
        "O estado terminal do ambiente √© indicado pela vari√°vel \"fim\" e, enquanto o valor dessa vari√°vel n√£o for `True`, os √∫ltimos dos passos descritos acima s√£o executados. No final de cada itera√ß√£o, deve-se receber do ambiente o pr√≥ximo estado, a recompensa que a a√ß√£o escolhida gerou, al√©m do sinal se estamos no estado terminal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdfE1mEvN0OU"
      },
      "source": [
        "## üë©‚Äçüíª Algoritmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoijK3v3N0OU"
      },
      "source": [
        "Primeiramente, precisaremos utilizar uma biblioteca chamada ***NumPy*** para auxiliar nas computa√ß√µes. Esta √© uma biblioteca do Python capaz de manusear diversas computa√ß√µes matem√°ticas com maestria e ser√° importante futuramente para o nosso trabalho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUiCG-Y1N0OU",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np # Importando a biblioteca NumPy\n",
        "import gymnasium as gym         # Importando a Biblioteca Gymnasium\n",
        "\n",
        "# Criando o nosso Ambiente\n",
        "env = gym.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)\n",
        "\n",
        "# N√∫mero total de a√ß√µes: 2\n",
        "# 0 = n√£o faz nada; 1 = bate as asas\n",
        "n_acoes = env.action_space.n\n",
        "\n",
        "print('N√∫mero de a√ß√µes:', n_acoes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpghWa-9N0OU"
      },
      "source": [
        "### üî¢ Discretizando o nosso Estado\n",
        "\n",
        "Como comentado anteriormente, o estado que o nosso agente recebe consiste das dist√¢ncias lidas pelo sensor. Dessa forma, uma breve partida de flappy bird pode conter in√∫meros estados, resultados da leitura do sensor a cada momento. \n",
        "\n",
        "O Q-Learning √© um algoritmo que guarda em uma tabela as estimativas do Q de cada a√ß√£o para cada estado.  esse gigantesco n√∫mero de estados exigiria n√£o somente guardar como atualizar cada um desses Q. N√£o √© uma situa√ß√£o ideal.\n",
        "\n",
        "Para simplificar (e agilizar) a situa√ß√£o, podemos \"discretizar\" os nossos estados. Faremos com que estados similares o suficiente sejam considerados como iguais e comparilhem das mesmas estimativas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fk1xN1VN0OV"
      },
      "outputs": [],
      "source": [
        "def discretiza_estado(estado):\n",
        "    return tuple(round(x/10) for x in estado)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6yw-iedN0OV"
      },
      "source": [
        "### üîÄ Escolhendo A√ß√µes\n",
        "\n",
        "Antes de iniciar o processo de escolha de a√ß√£o, √© necess√°rio entender dois conceitos essenciais para o aprendizado por refor√ßo:\n",
        "\n",
        "- **Explora√ß√£o**:√â a fase em que o agente est√° **explorando o ambiente**, isto √©, escolhendo a√ß√µes que ele n√£o costuma tomar para encontrar alguma solu√ß√£o que ele n√£o havia pensado antes.\n",
        "\n",
        "- **Explota√ß√£o**: Acontece quando o agente **aproveita** um conhecimento pr√©vio para tomar novas a√ß√µes que podem maximizar a recompensa recebida em cada epis√≥dio\n",
        "\n",
        "Nosso modelo precisa estabelecer um equil√≠brio entre **explorar e explotar**. Para isso, existem diversas estrat√©gias para alcan√ßar esse fim. Uma delas, √© a sele√ß√£o de a√ß√µes pela estrat√©gia do **\"$\\epsilon$-greedy\"**.\n",
        "\n",
        "#### A Estrat√©gia **$\\epsilon$-greedy**\n",
        "\n",
        "O algoritmo \"$\\epsilon$-greedy\" √© definido da seguinte forma: √© retirado um n√∫mero aleat√≥rio, no intervalo entre 0 e 1. caso este n√∫mero tenha valor inferior ao valor do epsilon, a escolha ser√° de uma a√ß√£o aleat√≥ria, o que configura explora√ß√£o. Caso este n√∫mero seja superior ao epsilon, a a√ß√£o a ser tomada √© a que gera a maior recompensa de acordo com os valores da tabela Q.\n",
        "\n",
        "Este valor de $\\epsilon$ n√£o √© constante ao longo do treinamento. Inicialmente, este valor √© alto, incentivando a maior explora√ß√£o do ambiente. A medida que o treinamento ocorre, mais informa√ß√£o sobre o ambiente √© adquirida, conseguindo uma tabela Q mais representativa da realidade. Dessa forma, quanto mais avan√ßado no treinamento, menor a necessidade de explora√ß√£o e maior a necessidade de exploitar o conhecimento adquirido para maximizar a recompensa. Esta atualiza√ß√£o do $\\epsilon$ √© chamada **\"$\\epsilon$-decay\"** (decaimento do epsilon). Tamb√©m √© estabelecido um valor m√≠nimo para o $\\epsilon$, para que o agente nunca pare completamente de explorar o ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTfMReirN0OV"
      },
      "outputs": [],
      "source": [
        "# Constantes da Pol√≠tica Epsilon Greedy\n",
        "# Epsilon: probabilidade de experimentar uma a√ß√£o aleat√≥ria\n",
        "EPSILON = 0.8        # Valor inicial do epsilon\n",
        "EPSILON_MIN = 0.01   # Valor m√≠nimo de epsilon\n",
        "DECAIMENTO = 0.9    # Fator de deca√≠mento do epsilon (por epis√≥dio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVPJeuKYN0OV"
      },
      "outputs": [],
      "source": [
        "def escolhe_acao(env, Q, estado, epsilon):\n",
        "    # Se n√£o conhecermos ainda o estado, inicializamos o Q de cada a√ß√£o como 0\n",
        "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
        "\n",
        "    # Escolhemos um n√∫mero aleat√≥rio com \"np.random.random()\"\n",
        "    # Se esse n√∫mero for menor que epsilon, tomamos uma a√ß√£o aleat√≥ria\n",
        "    if np.random.random() < epsilon:\n",
        "        # Escolhemos uma a√ß√£o aleat√≥ria, com env.action_space.sample()\n",
        "        acao = env.action_space.sample()\n",
        "    else:\n",
        "        # Escolhemos a melhor a√ß√£o para o estado atual, com np.argmax()\n",
        "        acao = np.argmax(Q)\n",
        "    return acao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBLy7qX3N0OV"
      },
      "source": [
        "Agora, finalizando a implementa√ß√£o de todos os passos descritos anteriormente, criamos a fun√ß√£o `roda_partida`, que recebe o ambiente e realiza todas as etapas necess√°rias para rodar uma partida, definidas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGQn7ZTVN0OV"
      },
      "outputs": [],
      "source": [
        "def roda_partida(env):\n",
        "    # Resetamos o ambiente\n",
        "    estado, _ = env.reset()\n",
        "\n",
        "    # Discretizamos o estado\n",
        "    estado = discretiza_estado(estado)\n",
        "\n",
        "    done = False\n",
        "    retorno = 0\n",
        "\n",
        "    while not done:\n",
        "        # Escolhemos uma a√ß√£o\n",
        "        acao = env.action_space.sample()\n",
        "\n",
        "        # Tomamos nossa a√ß√£o escolhida e recebemos informa√ß√µes do pr√≥ximo estado\n",
        "        prox_estado, recompensa, done, _, info = env.step(acao)\n",
        "\n",
        "        # Discretizamos o pr√≥ximo estado\n",
        "        prox_estado = discretiza_estado(prox_estado)\n",
        "\n",
        "        retorno += recompensa\n",
        "        estado = prox_estado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sF8oSqyDN0OV",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Rodamos uma partida\n",
        "roda_partida(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRUr2_7rN0OV"
      },
      "source": [
        "## üèãÔ∏è‚Äç‚ôÄÔ∏è Treinamento\n",
        "\n",
        "Agora sim chegaremos no treinamento propriamente dito. Usando os conceitos vistos na apresenta√ß√£o e nas se√ß√µes anteriores do notebook, podemos definir a fun√ß√£o de treinamento que vai permitir que o agente aprenda a jogar Flappy Bird por meio de **Q-Learning tabular**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "O pr√≥ximo passo √© definir uma estrat√©gia de treinamento do modelo, para que ele execute todos os passos definidos anteriormente de forma mais inteligente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGcPNqWiN0OV"
      },
      "source": [
        "O algoritmo se baseia na atualiza√ß√£o de estimativas dos valores Q para cada par estado-a√ß√£o, de forma a chegar a uma tabela cada vez mais pr√≥xima da realidade do ambiente. Dessa forma, devemos atualizar cada entrada da tabela de acordo com a **equa√ß√£o do Q-Learning**:\n",
        "\n",
        "$$Q*(s,a) \\leftarrow Q*(s,a) + \\alpha \\cdot \\left[r + \\gamma \\cdot \\max_{a'} (Q(s',a')) - Q(s, a)\\right]$$\n",
        "\n",
        "Esta equa√ß√£o corrige o valor do Q(s,a) de acordo com os valores anteriores somados a uma parcela de corre√ß√£o, de forma a minimizar o erro. A recompensa √© representada por r, enquanto os outros par√¢metros est√£o explicados a seguir:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXSyod-uN0OV"
      },
      "source": [
        "* \"ALFA\" ($\\alpha$): algoritmos de aprendizado de m√°quina costumam precisar de uma forma de serem otimizados. Q-learning trabalha em cima de gradientes, uma entidade matem√°tica que indica a dire√ß√£o para maximizar (ou minimizar) uma fun√ß√£o. Dispondo dessa dire√ß√£o, precisamos informar qual deve ser o tamanho do passo a ser dado antes de atualizar a nova \"dire√ß√£o ideal\".\n",
        "\n",
        "* \"GAMA\" ($\\gamma$): denota o quanto desejamos que nosso algoritmo considere eventos futuros. Se \"$\\gamma = 1$\", nosso algoritmo avaliar√° que a situa√ß√£o futura ser melhor que a atual √© t√£o importante quanto a recompensa da situa√ß√£o atual em si, por outro lado, se \"$\\gamma = 0$\", os eventos futuros n√£o apresentam import√¢ncia alguma para nosso algoritmo.\n",
        "\n",
        "* \"Q\" √© um dicion√°rio, ou seja, uma estrtura de dados capaz de buscar elementos de forma r√°pida. N√≥s o usaremos para guardar valores relativos √†s estimativas do algoritmo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbdQWL9cN0OV"
      },
      "outputs": [],
      "source": [
        "# Hiperpar√¢metros do Q-Learning\n",
        "ALFA = 0.001          # Learning rate\n",
        "GAMA = 0.98           # Fator de desconto\n",
        "\n",
        "# Dicion√°rio dos valores de Q\n",
        "# Chaves: estados; valores: qualidade Q atribuida a cada a√ß√£o\n",
        "Q = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXxvbLPmN0OV"
      },
      "outputs": [],
      "source": [
        "def atualiza_q(Q, estado, acao, recompensa, prox_estado):\n",
        "    # para cada estado ainda n√£o descoberto, iniciamos seu valor como nulo\n",
        "    if estado not in Q.keys(): Q[estado] = [0] * n_acoes\n",
        "    if prox_estado not in Q.keys(): Q[prox_estado] = [0] * n_acoes\n",
        "\n",
        "    # equa√ß√£o do Q-Learning\n",
        "    Q[estado][acao] = Q[estado][acao] + ALFA * (recompensa + GAMA*np.max(Q[prox_estado]) - Q[estado][acao])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9TNoRh0N0OV"
      },
      "source": [
        "Pickle √© uma maneira de salvar dados em um arquivo independente. Dessa forma, podemos gravar os valores da nossa tabela Q em um arquivo pr√≥prio, ficando dispon√≠vel para ser acessada em outro momento. Assim, podemos efetivamente salvar o modelo treinado para ser utilizado posteriormente. Abaixo, j√° est√£o presentes as fun√ß√µes de salvar e de abrir as tabelas com pickle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A62bPGFAN0OV"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def salva_tabela(Q, nome = 'model.pickle'):\n",
        "    with open(nome, 'wb') as pickle_out:\n",
        "        pickle.dump(Q, pickle_out)\n",
        "\n",
        "def carrega_tabela(nome = 'model.pickle'):\n",
        "    with open(nome, 'rb') as pickle_out:\n",
        "        return pickle.load(pickle_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQCI5KN3N0OV"
      },
      "source": [
        "A fun√ß√£o de treinamento tem estrutura semelhante √† fun√ß√£o roda_partida, conforme visto anteriormente. A cada epis√≥dio, o embiente deve ser reiniciado e discretizado, e deve indicar que o epis√≥dio ainda n√£o chegou em sua condi√ß√£o terminal. Devemos tamb√©m zerar o valor da recompensa, pois n√£o devemos utilizar o retorno do epis√≥dio anterior.\n",
        "\n",
        "Enquanto o epis√≥dio n√£o chega no final, o agente deve escolher uma a√ß√£o e tomar a a√ß√£o escolhida. Uma vez tomada a a√ß√£o, o ambiente fornece o pr√≥ximo estado, a recompensa recebida com a escolha, a indica√ß√£o se o estado √© terminal e informa√ß√µes sobre o ambiente.\n",
        "\n",
        "Em seguida, devemos discretizar o pr√≥ximo estado e atualizar os valores de q, o retorno e o estado atual.\n",
        "\n",
        "Por fim, devemos atualizar o valor do epsilon, de acordo com o m√©todo $\\epsilon$-greedy, onde deve ocorrer o decaimento do epsilon, mas seu valor nunca deve ser inferior ao valor m√≠nimo definido.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRXN4_OSN0OV"
      },
      "source": [
        "* `N_EPISODIOS` dita quantas vezes o agente dever√° \"reviver\" o ambiente (vit√≥rias e derrotas) antes de acabar seu treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBa1_tPyN0OW"
      },
      "outputs": [],
      "source": [
        "N_EPISODIOS = 120    # quantidade de epis√≥dios que treinaremos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qILw81AcN0OZ"
      },
      "outputs": [],
      "source": [
        "def treina(env, Q):\n",
        "    retornos = []      # retorno de cada epis√≥dio\n",
        "    epsilon = EPSILON\n",
        "\n",
        "    for episodio in range(1, N_EPISODIOS+1):\n",
        "        # resetar o ambiente\n",
        "        estado, _ = env.reset()\n",
        "        \n",
        "        # discretizar o estado inicial\n",
        "        estado = discretiza_estado(estado)\n",
        "        \n",
        "        done = False\n",
        "        retorno = 0\n",
        "        \n",
        "        while not done:\n",
        "            # politica\n",
        "            acao = escolhe_acao(env, Q, estado, epsilon)\n",
        "\n",
        "            # A a√ß√£o √© tomada e os valores novos s√£o coletados\n",
        "            # O novo estado √© salvo numa nova variavel\n",
        "            prox_estado, recompensa, done, _, info = env.step(acao)\n",
        "            prox_estado = discretiza_estado(prox_estado)\n",
        "\n",
        "            atualiza_q(Q, estado, acao, recompensa, prox_estado)\n",
        "\n",
        "            retorno += recompensa\n",
        "            estado = prox_estado\n",
        "\n",
        "        # atualiza o valor de epsilon para o pr√≥ximo epis√≥dio\n",
        "        epsilon = max(DECAIMENTO*epsilon, EPSILON_MIN)\n",
        "        retornos.append(retorno)\n",
        "\n",
        "        if episodio % 10 == 0:\n",
        "            salva_tabela(Q)\n",
        "\n",
        "        # log do resultado dos √∫ltimos epis√≥dios\n",
        "        print(f'epis√≥dio {episodio},  '\n",
        "              f'retorno {retorno:7.1f},  '\n",
        "              f'retorno m√©dio (√∫ltimos 10 epis√≥dios) {np.mean(retornos[-10:]):7.1f},  '\n",
        "              f'epsilon: {epsilon:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de testa a fun√ß√£o de treino, ser√° necess√°rio inicializar um novo ambiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AwQa1QzN0OZ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "treina(env, Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tZyi6rqN0OZ"
      },
      "source": [
        "## Testando nosso Agente Treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roda_partida(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Encerramos o ambienteclose\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
